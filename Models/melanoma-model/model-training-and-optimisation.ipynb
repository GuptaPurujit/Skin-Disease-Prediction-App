{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031097,
     "end_time": "2020-09-21T18:07:49.255213",
     "exception": false,
     "start_time": "2020-09-21T18:07:49.224116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Variables and Dependencies\n",
    "- tfrec_shape = image shape to choose dataset 256, 384, 512, 768\n",
    "- comp_data = \"2020\"(only 2020 competition data) or \"2019-2020\"(2017-2020 data)\n",
    "- hair_augm = bool (whether hair augment is applied)\n",
    "- Coarse Dropout Variables:\n",
    "  * DROP_FREQ Determines proportion of train images to apply coarse dropout to/ (0,1)\n",
    "  * DROP_CT How many squares to remove from train images when applying dropout\n",
    "  * DROP_SIZE The size of square side equals IMG_SIZE * DROP_SIZE / (0,1)\n",
    "- Transform Augmentation Variables:\n",
    "  * rot, rotation\n",
    "  * shr, shear zoom\n",
    "  * hzoom, height zoom\n",
    "  * wzoom, width shift\n",
    "  * hshift, height shift\n",
    "  * wshift, width shift\n",
    "- crop_size random crop size for each original image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:07:49.327275Z",
     "iopub.status.busy": "2020-09-21T18:07:49.326125Z",
     "iopub.status.idle": "2020-09-21T18:07:49.330947Z",
     "shell.execute_reply": "2020-09-21T18:07:49.330206Z"
    },
    "papermill": {
     "duration": 0.049674,
     "end_time": "2020-09-21T18:07:49.331094",
     "exception": false,
     "start_time": "2020-09-21T18:07:49.281420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfrec_shape = 256\n",
    "comp_data = \"2019-2020\"\n",
    "crop_size = {256: 250, 384: 370, 512: 500, 768: 750}\n",
    "\n",
    "if comp_data == \"2020\":\n",
    "    net_size = {256: 248, 384: 370, 512: 500, 768: 750}\n",
    "elif comp_data == \"2019-2020\":\n",
    "    net_size = {256: 250, 384: 370, 512: 500, 768: 750}\n",
    "\n",
    "# hair augmentation\n",
    "if comp_data == \"2020\":\n",
    "    hair_augm = {256: False, 384: False, 512: False, 768: False}\n",
    "elif comp_data == \"2019-2020\":\n",
    "    hair_augm = {256: True, 384: True, 512: True, 768: False}\n",
    "    \n",
    "# epochs\n",
    "if comp_data == \"2020\":\n",
    "    epochs_num = {256: 13, 384: 15, 512: 15, 768: 15}\n",
    "elif comp_data == \"2019-2020\":\n",
    "    epochs_num = {256: 32, 384: 25, 512: 16, 768: 10}\n",
    "\n",
    "# model weights\n",
    "model_weights = 'imagenet' # 'noisy-student'\n",
    "\n",
    "# device\n",
    "DEVICE = \"TPU\" #TPU or GPU\n",
    "\n",
    "# Edit to change Model Aug/Hyperparams\n",
    "\n",
    "CFG = dict(\n",
    "            batch_size = 16,\n",
    "            read_size = tfrec_shape,\n",
    "            crop_size = crop_size[tfrec_shape],\n",
    "            net_size = net_size[tfrec_shape],\n",
    "    \n",
    "            # LEARNING RATE\n",
    "            LR_START = 0.000003,\n",
    "            LR_MAX = 0.000020,\n",
    "            LR_MIN = 0.000001,\n",
    "            LR_RAMPUP_EPOCHS  = 5,\n",
    "            LR_SUSTAIN_EPOCHS = 0,\n",
    "            LR_EXP_DECAY = 0.8,\n",
    "    \n",
    "            # EPOCHS:\n",
    "            epochs = epochs_num[tfrec_shape],\n",
    "    \n",
    "            # DATA AUGMENTATION\n",
    "            rot = 180.0,\n",
    "            shr = 1.5,\n",
    "            hzoom = 6.0,\n",
    "            wzoom = 6.0,\n",
    "            hshift = 6.0,\n",
    "            wshift = 6.0,\n",
    "    \n",
    "            # COARSE DROPOUT\n",
    "            DROP_FREQ = 0.86, \n",
    "            DROP_CT = 16, \n",
    "            DROP_SIZE = 0.065,   \n",
    "    \n",
    "            # HAIR AUGMENTATION:\n",
    "            hair_augm = hair_augm[tfrec_shape],\n",
    "    \n",
    "            optimizer = 'adam',\n",
    "            label_smooth_fac = 0.05,\n",
    "            tta_steps =  25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:07:49.397633Z",
     "iopub.status.busy": "2020-09-21T18:07:49.396785Z",
     "iopub.status.idle": "2020-09-21T18:08:22.529548Z",
     "shell.execute_reply": "2020-09-21T18:08:22.528874Z"
    },
    "papermill": {
     "duration": 33.171641,
     "end_time": "2020-09-21T18:08:22.529692",
     "exception": false,
     "start_time": "2020-09-21T18:07:49.358051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! /opt/conda/bin/python3.7 -m pip install -q --upgrade pip\n",
    "! pip install -q efficientnet\n",
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025653,
     "end_time": "2020-09-21T18:08:22.581500",
     "exception": false,
     "start_time": "2020-09-21T18:08:22.555847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:22.643974Z",
     "iopub.status.busy": "2020-09-21T18:08:22.643179Z",
     "iopub.status.idle": "2020-09-21T18:08:31.909256Z",
     "shell.execute_reply": "2020-09-21T18:08:31.908169Z"
    },
    "papermill": {
     "duration": 9.301871,
     "end_time": "2020-09-21T18:08:31.909402",
     "exception": false,
     "start_time": "2020-09-21T18:08:22.607531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, re, math, time\n",
    "random.seed(a=42)\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import efficientnet.tfkeras as efn\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import classification_report\n",
    "import PIL\n",
    "\n",
    "#import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025431,
     "end_time": "2020-09-21T18:08:31.964265",
     "exception": false,
     "start_time": "2020-09-21T18:08:31.938834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:32.033746Z",
     "iopub.status.busy": "2020-09-21T18:08:32.032939Z",
     "iopub.status.idle": "2020-09-21T18:08:34.198430Z",
     "shell.execute_reply": "2020-09-21T18:08:34.197761Z"
    },
    "papermill": {
     "duration": 2.208397,
     "end_time": "2020-09-21T18:08:34.198563",
     "exception": false,
     "start_time": "2020-09-21T18:08:31.990166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASEPATH = \"../input/siim-isic-melanoma-classification\"\n",
    "df_train = pd.read_csv(os.path.join(BASEPATH, 'train.csv'))\n",
    "df_test  = pd.read_csv(os.path.join(BASEPATH, 'test.csv'))\n",
    "df_sub   = pd.read_csv(os.path.join(BASEPATH, 'sample_submission.csv'))\n",
    "\n",
    "# 2020 TFRecords\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path(f'melanoma-{tfrec_shape}x{tfrec_shape}')\n",
    "\n",
    "# 2019 TFRecords\n",
    "GCS_PATH_2019 = KaggleDatasets().get_gcs_path(f'isic2019-{tfrec_shape}x{tfrec_shape}')\n",
    "\n",
    "# TRAIN\n",
    "if comp_data == \"2020\":\n",
    "    files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')))\n",
    "elif comp_data == \"2019-2020\":\n",
    "    ## 2020 + 2019 (all, including 2017+2018):\n",
    "    files_train = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "    files_train += tf.io.gfile.glob(GCS_PATH_2019 + '/train*.tfrec')\n",
    "    files_train = np.sort(np.array(files_train)) # np.random.shuffle(files_train)\n",
    "\n",
    "\n",
    "# TEST\n",
    "files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')))\n",
    "\n",
    "# loading hairs\n",
    "GCS_PATH_hair_images = KaggleDatasets().get_gcs_path('melanoma-hairs')\n",
    "hair_images = tf.io.gfile.glob(GCS_PATH_hair_images + '/*.png')\n",
    "hair_images_tf=tf.convert_to_tensor(hair_images)\n",
    "\n",
    "# the maximum number of hairs to augment:\n",
    "n_max= 20\n",
    "\n",
    "# Scaling factor:\n",
    "if tfrec_shape != 256:\n",
    "    scale=tf.cast(CFG['crop_size']/256, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025481,
     "end_time": "2020-09-21T18:08:34.249984",
     "exception": false,
     "start_time": "2020-09-21T18:08:34.224503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TPU configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:34.314910Z",
     "iopub.status.busy": "2020-09-21T18:08:34.314131Z",
     "iopub.status.idle": "2020-09-21T18:08:39.570510Z",
     "shell.execute_reply": "2020-09-21T18:08:39.569769Z"
    },
    "papermill": {
     "duration": 5.294698,
     "end_time": "2020-09-21T18:08:39.570660",
     "exception": false,
     "start_time": "2020-09-21T18:08:34.275962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "initializing  TPU ...\n",
      "TPU initialized\n",
      "REPLICAS: 8\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027303,
     "end_time": "2020-09-21T18:08:39.625795",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.598492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions To implement Augmentation:\n",
    "\n",
    "* Hair removal\n",
    "* Coarse Dropout\n",
    "* Image Transforms\n",
    "* Crop Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:39.712102Z",
     "iopub.status.busy": "2020-09-21T18:08:39.711051Z",
     "iopub.status.idle": "2020-09-21T18:08:39.715341Z",
     "shell.execute_reply": "2020-09-21T18:08:39.714448Z"
    },
    "papermill": {
     "duration": 0.062502,
     "end_time": "2020-09-21T18:08:39.715505",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.653003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================== FUNCTION TO PERFORM HAIR REMOVAL ON IMAGE ====================================\n",
    "\n",
    "def hair_aug_tf(input_img, augment=True):\n",
    "    \n",
    "    if augment:\n",
    "    \n",
    "        # Copy the input image, so it won't be changed\n",
    "        img = tf.identity(input_img)\n",
    "\n",
    "        # Unnormalize: Returning the image from 0-1 to 0-255:\n",
    "        img = tf.multiply(img, 255)\n",
    "\n",
    "        # Randomly choose the number of hairs to augment (up to n_max)\n",
    "        n_hairs = tf.random.uniform(shape=[], maxval=tf.constant(n_max)+1,dtype=tf.int32)\n",
    "\n",
    "        im_height = tf.shape(img)[0]\n",
    "        im_width = tf.shape(img)[1]\n",
    "\n",
    "        if n_hairs == 0:\n",
    "            # Normalize the image to [0,1]\n",
    "            img = tf.multiply(img, 1/255)\n",
    "            return img\n",
    "\n",
    "        for _ in tf.range(n_hairs):\n",
    "\n",
    "            # Read a random hair image\n",
    "            i = tf.random.uniform(shape=[], maxval=tf.shape(hair_images_tf)[0],dtype=tf.int32)\n",
    "            fname = hair_images_tf[i]\n",
    "            bits = tf.io.read_file(fname)\n",
    "            hair = tf.image.decode_jpeg(bits)\n",
    "\n",
    "            # Rescale the hair image to the right size\n",
    "            if tfrec_shape != 256:\n",
    "                # new_height, new_width, _  = scale*tf.shape(hair)\n",
    "                new_width = scale*tf.shape(hair)[1]\n",
    "                new_height = scale*tf.shape(hair)[0]\n",
    "                hair = tf.image.resize(hair, [new_height, new_width])\n",
    "\n",
    "            # Random flips of the hair image\n",
    "            hair = tf.image.random_flip_left_right(hair)\n",
    "            hair = tf.image.random_flip_up_down(hair)\n",
    "\n",
    "            # Random number of 90 degree rotations\n",
    "            n_rot = tf.random.uniform(shape=[], maxval=4,dtype=tf.int32)\n",
    "            hair = tf.image.rot90(hair, k=n_rot)\n",
    "\n",
    "            # The hair image height and width (ignore the number of color channels)\n",
    "            h_height = tf.shape(hair)[0]\n",
    "            h_width = tf.shape(hair)[1]\n",
    "\n",
    "            # The top left coord's of the region of interest (roi) where the augmentation will be performed\n",
    "            roi_h0 = tf.random.uniform(shape=[], maxval=im_height - h_height + 1, dtype=tf.int32)\n",
    "            roi_w0 = tf.random.uniform(shape=[], maxval=im_width - h_width + 1, dtype=tf.int32)\n",
    "\n",
    "            # The region of interest\n",
    "            roi = img[roi_h0:(roi_h0 + h_height), roi_w0:(roi_w0 + h_width)]  \n",
    "\n",
    "            # Convert the hair image to grayscale (slice to remove the trainsparency channel)\n",
    "            hair2gray = tf.image.rgb_to_grayscale(hair[:, :, :3])\n",
    "\n",
    "            # Threshold:\n",
    "            mask = hair2gray>10\n",
    "\n",
    "            img_bg = tf.multiply(roi, tf.cast(tf.image.grayscale_to_rgb(~mask), dtype=tf.float32))\n",
    "            hair_fg = tf.multiply(tf.cast(hair[:, :, :3], dtype=tf.int32), tf.cast(tf.image.grayscale_to_rgb(mask), dtype=tf.int32))\n",
    "\n",
    "            dst = tf.add(img_bg, tf.cast(hair_fg, dtype=tf.float32))\n",
    "\n",
    "            paddings = tf.stack([[roi_h0, im_height-(roi_h0 + h_height)], [roi_w0, im_width-(roi_w0 + h_width)],[0, 0]])\n",
    "            # Pad dst with zeros to make it the same shape as image.\n",
    "            dst_padded=tf.pad(dst, paddings, \"CONSTANT\")\n",
    "\n",
    "            # Create a boolean mask with zeros at the pixels of the augmentation segment and ones everywhere else\n",
    "            mask_img=tf.pad(tf.ones_like(dst), paddings, \"CONSTANT\")\n",
    "            mask_img=~tf.cast(mask_img, dtype=tf.bool)\n",
    "\n",
    "            # Make a hole in the original image at the location of the augmentation segment\n",
    "            img_hole=tf.multiply(img, tf.cast(mask_img, dtype=tf.float32))\n",
    "\n",
    "            # Inserting the augmentation segment in place of the hole\n",
    "            img = tf.add(img_hole, dst_padded)\n",
    "\n",
    "        # Normalize the image to [0,1]\n",
    "        img = tf.multiply(img, 1/255)\n",
    "        \n",
    "        return img\n",
    "    else:\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:39.789723Z",
     "iopub.status.busy": "2020-09-21T18:08:39.788540Z",
     "iopub.status.idle": "2020-09-21T18:08:39.792902Z",
     "shell.execute_reply": "2020-09-21T18:08:39.792182Z"
    },
    "papermill": {
     "duration": 0.050126,
     "end_time": "2020-09-21T18:08:39.793034",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.742908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#===================== FUNCTION TO PERFORM COARSE AUMENTATION ON IMAGE =============================\n",
    "\n",
    "\n",
    "def dropout(image, DIM=256, PROBABILITY = 0.75, CT = 8, SZ = 0.2):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "    \n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n",
    "    if (P==0)|(CT==0)|(SZ==0): return image\n",
    "    \n",
    "    for k in range(CT):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        # COMPUTE SQUARE\n",
    "        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3]) \n",
    "        three = image[ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n",
    "            \n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR\n",
    "    image = tf.reshape(image,[DIM,DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:39.880242Z",
     "iopub.status.busy": "2020-09-21T18:08:39.879052Z",
     "iopub.status.idle": "2020-09-21T18:08:39.882678Z",
     "shell.execute_reply": "2020-09-21T18:08:39.881921Z"
    },
    "papermill": {
     "duration": 0.061867,
     "end_time": "2020-09-21T18:08:39.882828",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.820961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#============================ ROTATION, SHEAR, ZOOM, TRANSFORMS ====================================\n",
    "\n",
    "# FUNCTION TO GET ROTATED MATRIX:\n",
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "# FUNCTION TO APPLY AUGMENTATION TRANSFORMS:\n",
    "\n",
    "def transform(image, cfg):    \n",
    "    # input image - [dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = cfg[\"read_size\"]\n",
    "    XDIM = DIM%2\n",
    "    \n",
    "    rot = cfg['rot'] * tf.random.normal([1], dtype='float32')\n",
    "    shr = cfg['shr'] * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['hzoom']\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['wzoom']\n",
    "    h_shift = cfg['hshift'] * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = cfg['wshift'] * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift)\n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:39.954629Z",
     "iopub.status.busy": "2020-09-21T18:08:39.953496Z",
     "iopub.status.idle": "2020-09-21T18:08:39.957528Z",
     "shell.execute_reply": "2020-09-21T18:08:39.956722Z"
    },
    "papermill": {
     "duration": 0.046782,
     "end_time": "2020-09-21T18:08:39.957676",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.910894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#========================== FUNCTION TO READ TFRECORD FILEPATHS ====================================\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n",
    "        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }           \n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['target']\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_name):\n",
    "    tfrec_format = {\n",
    "        'image'                        : tf.io.FixedLenFeature([], tf.string),\n",
    "        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return example['image'], example['image_name'] if return_image_name else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:40.029572Z",
     "iopub.status.busy": "2020-09-21T18:08:40.028755Z",
     "iopub.status.idle": "2020-09-21T18:08:40.033463Z",
     "shell.execute_reply": "2020-09-21T18:08:40.032556Z"
    },
    "papermill": {
     "duration": 0.047882,
     "end_time": "2020-09-21T18:08:40.033612",
     "exception": false,
     "start_time": "2020-09-21T18:08:39.985730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#====================== FUNCTION TO APPLY AUGMENTATION TO EACH IMAGE ===============================\n",
    "\n",
    "def prepare_image(img, cfg=None, augment=True):\n",
    "    \n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [cfg['read_size'], cfg['read_size']])\n",
    "    img = tf.cast(img, tf.float32) / 255.0 # # Cast and normalize the image to [0,1]\n",
    "    \n",
    "    if augment:\n",
    "        \n",
    "        # Data augmentation\n",
    "        img = transform(img, cfg)\n",
    "        img = tf.image.random_crop(img, [cfg['crop_size'], cfg['crop_size'], 3]) \n",
    "        # Coarse dropout\n",
    "        # img = dropout(img, DIM=cfg['crop_size'], PROBABILITY=cfg['DROP_FREQ'], CT=cfg['DROP_CT'], SZ=cfg['DROP_SIZE'])\n",
    "        # Other augmentations\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1)\n",
    "        # Hair augmentation\n",
    "        img = hair_aug_tf(img, augment=cfg['hair_augm'])\n",
    "    else:\n",
    "        img = tf.image.central_crop(img, cfg['crop_size'] / cfg['read_size'])\n",
    "                                   \n",
    "    img = tf.image.resize(img, [cfg['net_size'], cfg['net_size']])\n",
    "    img = tf.reshape(img, [cfg['net_size'], cfg['net_size'], 3])        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:40.106444Z",
     "iopub.status.busy": "2020-09-21T18:08:40.105354Z",
     "iopub.status.idle": "2020-09-21T18:08:40.108853Z",
     "shell.execute_reply": "2020-09-21T18:08:40.108147Z"
    },
    "papermill": {
     "duration": 0.047333,
     "end_time": "2020-09-21T18:08:40.108993",
     "exception": false,
     "start_time": "2020-09-21T18:08:40.061660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================== FUNCTIONS TO OBTAIN IMAGES FROM TFRECORD FILEPATHS ============================\n",
    "\n",
    "# COUNT NUMBER OF IMAGES IN FILEPATH:\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "# OBTAIN DATASET:\n",
    "def get_dataset(files, \n",
    "                cfg, augment = False, \n",
    "                shuffle = False, repeat = False, \n",
    "                labeled=True, return_image_names=True):\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    ds = ds.cache()\n",
    "    \n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "        \n",
    "    if labeled: \n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), num_parallel_calls=AUTO)      \n",
    "    \n",
    "    ds = ds.map(lambda img, imgname_or_label: (prepare_image(img, augment=augment, cfg=cfg),imgname_or_label), num_parallel_calls=AUTO)\n",
    "    \n",
    "    ds = ds.batch(cfg['batch_size'] * REPLICAS)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027639,
     "end_time": "2020-09-21T18:08:40.164701",
     "exception": false,
     "start_time": "2020-09-21T18:08:40.137062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Obtaining datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:40.229166Z",
     "iopub.status.busy": "2020-09-21T18:08:40.228151Z",
     "iopub.status.idle": "2020-09-21T18:08:40.233062Z",
     "shell.execute_reply": "2020-09-21T18:08:40.232162Z"
    },
    "papermill": {
     "duration": 0.040352,
     "end_time": "2020-09-21T18:08:40.233211",
     "exception": false,
     "start_time": "2020-09-21T18:08:40.192859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 57964 training images, 10982 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "# IMAGES IN DATASET:\n",
    "num_training_images = int(count_data_items(files_train))\n",
    "num_test_images = count_data_items(files_test)\n",
    "print('Dataset: {} training images, {} unlabeled test images'.format(\n",
    "num_training_images, num_test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:40.303530Z",
     "iopub.status.busy": "2020-09-21T18:08:40.302374Z",
     "iopub.status.idle": "2020-09-21T18:08:43.792271Z",
     "shell.execute_reply": "2020-09-21T18:08:43.792867Z"
    },
    "papermill": {
     "duration": 3.531071,
     "end_time": "2020-09-21T18:08:43.793039",
     "exception": false,
     "start_time": "2020-09-21T18:08:40.261968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = get_dataset(files_train, CFG, augment=True, shuffle=True, repeat=True)\n",
    "ds_train = ds_train.map(lambda img, label: (img, tuple([label])))\n",
    "steps_train = count_data_items(files_train) / (CFG['batch_size'] * REPLICAS)\n",
    "#ds_test = get_dataset(files_test, CFG, labeled=False)\n",
    "\n",
    "test_images = ds_train.take(75)\n",
    "ds_train = ds_train.skip(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:43.862362Z",
     "iopub.status.busy": "2020-09-21T18:08:43.860965Z",
     "iopub.status.idle": "2020-09-21T18:08:43.865210Z",
     "shell.execute_reply": "2020-09-21T18:08:43.864601Z"
    },
    "papermill": {
     "duration": 0.041735,
     "end_time": "2020-09-21T18:08:43.865400",
     "exception": false,
     "start_time": "2020-09-21T18:08:43.823665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    plt.plot(history['auc'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['ROC_AUC', 'loss'], loc = 'upper right')\n",
    "    plt.title('ROC_AUC vs loss - train vs test')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('AUC and loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028943,
     "end_time": "2020-09-21T18:08:43.924477",
     "exception": false,
     "start_time": "2020-09-21T18:08:43.895534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Building and Compiling Model:\n",
    "\n",
    "### Model Details:\n",
    "- Losses used:\n",
    "    * Binary Focal Loss\n",
    "    * Binary Cross Entropy\n",
    "- Optimizer: Adam\n",
    "- Custom LR Callback\n",
    "- Model Checkpoint Callback\n",
    "- Save Model and Infer\n",
    "- Notes On Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:43.999818Z",
     "iopub.status.busy": "2020-09-21T18:08:43.998688Z",
     "iopub.status.idle": "2020-09-21T18:08:44.002380Z",
     "shell.execute_reply": "2020-09-21T18:08:44.001701Z"
    },
    "papermill": {
     "duration": 0.048327,
     "end_time": "2020-09-21T18:08:44.002524",
     "exception": false,
     "start_time": "2020-09-21T18:08:43.954197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LEARNING RATE SCHEDULER\n",
    "\n",
    "def get_lr_callback(cfg):\n",
    "    lr_start = cfg['LR_START']\n",
    "    lr_max = cfg['LR_MAX'] * strategy.num_replicas_in_sync\n",
    "    lr_min = cfg['LR_MIN']\n",
    "    lr_ramp_ep = cfg['LR_RAMPUP_EPOCHS']\n",
    "    lr_sus_ep = cfg['LR_SUSTAIN_EPOCHS']\n",
    "    lr_decay = cfg['LR_EXP_DECAY']\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:44.077076Z",
     "iopub.status.busy": "2020-09-21T18:08:44.076324Z",
     "iopub.status.idle": "2020-09-21T18:08:44.080871Z",
     "shell.execute_reply": "2020-09-21T18:08:44.080120Z"
    },
    "papermill": {
     "duration": 0.04925,
     "end_time": "2020-09-21T18:08:44.081005",
     "exception": false,
     "start_time": "2020-09-21T18:08:44.031755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BUILD MODEL\n",
    "def get_model(cfg, model):\n",
    "    \n",
    "    model_input = tf.keras.Input(shape=(cfg['net_size'], cfg['net_size'], 3), name='imgIn')\n",
    "    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)\n",
    "    outputs = []\n",
    "\n",
    "    constructor = getattr(efn, model)\n",
    "    x = constructor(include_top=False, weights=model_weights, input_shape=(cfg['net_size'], cfg['net_size'], 3), pooling='avg')(dummy)\n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    outputs.append(x)\n",
    "    \n",
    "    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# COMPILE MODEL\n",
    "def compile_new_model(cfg, model):\n",
    "    with strategy.scope():\n",
    "        model = get_model(cfg, model)\n",
    "     \n",
    "        losses = tf.keras.losses.BinaryCrossentropy(label_smoothing = cfg['label_smooth_fac'])\n",
    "\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer = cfg['optimizer'],\n",
    "            loss = losses,\n",
    "            metrics = [tf.keras.metrics.AUC(name='auc')]) \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02906,
     "end_time": "2020-09-21T18:08:44.139461",
     "exception": false,
     "start_time": "2020-09-21T18:08:44.110401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Effnet B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T18:08:44.207545Z",
     "iopub.status.busy": "2020-09-21T18:08:44.206784Z",
     "iopub.status.idle": "2020-09-21T19:17:18.926883Z",
     "shell.execute_reply": "2020-09-21T19:17:18.926194Z"
    },
    "papermill": {
     "duration": 4114.758278,
     "end_time": "2020-09-21T19:17:18.927049",
     "exception": false,
     "start_time": "2020-09-21T18:08:44.168771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "115515392/115515256 [==============================] - 4s 0us/step\n",
      "Model: \"aNetwork\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "imgIn (InputLayer)           [(None, 250, 250, 3)]     0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 250, 250, 3)       0         \n",
      "_________________________________________________________________\n",
      "efficientnet-b5 (Model)      (None, 2048)              28513520  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 28,515,569\n",
      "Trainable params: 28,342,833\n",
      "Non-trainable params: 172,736\n",
      "_________________________________________________________________\n",
      "\n",
      " Begin Training Models\n",
      "Epoch 1/32\n",
      "453/452 [==============================] - 122s 268ms/step - loss: 0.5133 - auc: 0.6389 - lr: 3.0000e-06\n",
      "Epoch 2/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2987 - auc: 0.8218 - lr: 3.4400e-05\n",
      "Epoch 3/32\n",
      "453/452 [==============================] - 121s 268ms/step - loss: 0.2812 - auc: 0.8575 - lr: 6.5800e-05\n",
      "Epoch 4/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2722 - auc: 0.8760 - lr: 9.7200e-05\n",
      "Epoch 5/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2639 - auc: 0.8906 - lr: 1.2860e-04\n",
      "Epoch 6/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2595 - auc: 0.8995 - lr: 1.6000e-04\n",
      "Epoch 7/32\n",
      "453/452 [==============================] - 121s 268ms/step - loss: 0.2517 - auc: 0.9110 - lr: 1.2820e-04\n",
      "Epoch 8/32\n",
      "453/452 [==============================] - 122s 268ms/step - loss: 0.2473 - auc: 0.9181 - lr: 1.0276e-04\n",
      "Epoch 9/32\n",
      "453/452 [==============================] - 121s 268ms/step - loss: 0.2423 - auc: 0.9241 - lr: 8.2408e-05\n",
      "Epoch 10/32\n",
      "453/452 [==============================] - 121s 268ms/step - loss: 0.2378 - auc: 0.9301 - lr: 6.6126e-05\n",
      "Epoch 11/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2341 - auc: 0.9325 - lr: 5.3101e-05\n",
      "Epoch 12/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2301 - auc: 0.9371 - lr: 4.2681e-05\n",
      "Epoch 13/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2291 - auc: 0.9396 - lr: 3.4345e-05\n",
      "Epoch 14/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2269 - auc: 0.9411 - lr: 2.7676e-05\n",
      "Epoch 15/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2213 - auc: 0.9476 - lr: 2.2341e-05\n",
      "Epoch 16/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2227 - auc: 0.9463 - lr: 1.8072e-05\n",
      "Epoch 17/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2212 - auc: 0.9474 - lr: 1.4658e-05\n",
      "Epoch 18/32\n",
      "453/452 [==============================] - 121s 268ms/step - loss: 0.2199 - auc: 0.9487 - lr: 1.1926e-05\n",
      "Epoch 19/32\n",
      "453/452 [==============================] - 120s 266ms/step - loss: 0.2184 - auc: 0.9499 - lr: 9.7411e-06\n",
      "Epoch 20/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2182 - auc: 0.9521 - lr: 7.9929e-06\n",
      "Epoch 21/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2183 - auc: 0.9505 - lr: 6.5943e-06\n",
      "Epoch 22/32\n",
      "453/452 [==============================] - 121s 267ms/step - loss: 0.2169 - auc: 0.9516 - lr: 5.4755e-06\n",
      "Epoch 23/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2167 - auc: 0.9517 - lr: 4.5804e-06\n",
      "Epoch 24/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2149 - auc: 0.9536 - lr: 3.8643e-06\n",
      "Epoch 25/32\n",
      "453/452 [==============================] - 121s 266ms/step - loss: 0.2165 - auc: 0.9530 - lr: 3.2914e-06\n",
      "Epoch 26/32\n",
      "453/452 [==============================] - 120s 265ms/step - loss: 0.2155 - auc: 0.9535 - lr: 2.8331e-06\n",
      "Epoch 27/32\n",
      "453/452 [==============================] - 120s 266ms/step - loss: 0.2147 - auc: 0.9541 - lr: 2.4665e-06\n",
      "Epoch 28/32\n",
      "453/452 [==============================] - 120s 266ms/step - loss: 0.2150 - auc: 0.9540 - lr: 2.1732e-06\n",
      "Epoch 29/32\n",
      "453/452 [==============================] - 120s 266ms/step - loss: 0.2154 - auc: 0.9541 - lr: 1.9386e-06\n",
      "Epoch 30/32\n",
      "453/452 [==============================] - 120s 265ms/step - loss: 0.2153 - auc: 0.9517 - lr: 1.7509e-06\n",
      "Epoch 31/32\n",
      "453/452 [==============================] - 120s 265ms/step - loss: 0.2146 - auc: 0.9532 - lr: 1.6007e-06\n",
      "Epoch 32/32\n",
      "453/452 [==============================] - 120s 266ms/step - loss: 0.2131 - auc: 0.9565 - lr: 1.4805e-06\n",
      "\n",
      " Done Training model_B5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_B5 = compile_new_model(CFG, 'EfficientNetB5')\n",
    "\n",
    "print(\"\\n Begin Training Models\")\n",
    "history_B5 = model_B5.fit(ds_train, \n",
    "                          verbose=1, \n",
    "                          steps_per_epoch=steps_train, \n",
    "                          epochs = CFG['epochs'], \n",
    "                          callbacks=[get_lr_callback(CFG), \n",
    "                                    tfmot.sparsity.keras.UpdatePruningStep()]) \n",
    "\n",
    "print(\"\\n Done Training model_B5 \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:17:29.467884Z",
     "iopub.status.busy": "2020-09-21T19:17:29.466944Z",
     "iopub.status.idle": "2020-09-21T19:17:29.765427Z",
     "shell.execute_reply": "2020-09-21T19:17:29.765939Z"
    },
    "papermill": {
     "duration": 5.610291,
     "end_time": "2020-09-21T19:17:29.766126",
     "exception": false,
     "start_time": "2020-09-21T19:17:24.155835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy_loss(history_B5.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:17:40.054351Z",
     "iopub.status.busy": "2020-09-21T19:17:40.053510Z",
     "iopub.status.idle": "2020-09-21T19:17:51.234991Z",
     "shell.execute_reply": "2020-09-21T19:17:51.234306Z"
    },
    "papermill": {
     "duration": 16.259509,
     "end_time": "2020-09-21T19:17:51.235151",
     "exception": false,
     "start_time": "2020-09-21T19:17:34.975642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model:\n",
    "model_B5.save(f\"./EfficientNetB5_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG['epochs']}_auc_{round(history_B5.history['auc'][CFG['epochs']-1], 2)}.h5\")\n",
    "# Serialize weights to h5:\n",
    "model_B5.save_weights(f\"./EfficientNetB5_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG['epochs']}_auc_{round(history_B5.history['auc'][CFG['epochs']-1], 2)}_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.401528,
     "end_time": "2020-09-21T19:18:01.847499",
     "exception": false,
     "start_time": "2020-09-21T19:17:56.445971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train Effnet B6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:18:12.402505Z",
     "iopub.status.busy": "2020-09-21T19:18:12.401313Z",
     "iopub.status.idle": "2020-09-21T19:18:12.406908Z",
     "shell.execute_reply": "2020-09-21T19:18:12.407480Z"
    },
    "papermill": {
     "duration": 5.19025,
     "end_time": "2020-09-21T19:18:12.407650",
     "exception": false,
     "start_time": "2020-09-21T19:18:07.217400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_B6 = compile_new_model(CFG, \\'EfficientNetB6\\')\\n\\nprint(\"\\n Begin Training Models\")\\nhistory_B6 = model_B6.fit(ds_train, \\n                          verbose=1, \\n                          steps_per_epoch=steps_train, \\n                          epochs = CFG[\\'epochs\\'], \\n                          callbacks=[get_lr_callback(CFG), \\n                                    tfmot.sparsity.keras.UpdatePruningStep()]) \\n\\nprint(\"\\n Done Training model_B6 \\n\")\\nplot_accuracy_loss(history_B6.history)\\n# Save model:\\nmodel_B6.save(f\"./EfficientNetB6_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG[\\'epochs\\']}_auc_{round(history_B6.history[\\'auc\\'][CFG[\\'epochs\\']-1], 2)}.h5\")\\n# Serialize weights to h5:\\nmodel_B6.save_weights(f\"./EfficientNetB6_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG[\\'epochs\\']}_auc_{round(history_B6.history[\\'auc\\'][CFG[\\'epochs\\']-1], 2)}_weights.h5\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_B6 = compile_new_model(CFG, 'EfficientNetB6')\n",
    "\n",
    "print(\"\\n Begin Training Models\")\n",
    "history_B6 = model_B6.fit(ds_train, \n",
    "                          verbose=1, \n",
    "                          steps_per_epoch=steps_train, \n",
    "                          epochs = CFG['epochs'], \n",
    "                          callbacks=[get_lr_callback(CFG), \n",
    "                                    tfmot.sparsity.keras.UpdatePruningStep()]) \n",
    "\n",
    "print(\"\\n Done Training model_B6 \\n\")\n",
    "plot_accuracy_loss(history_B6.history)\n",
    "# Save model:\n",
    "model_B6.save(f\"./EfficientNetB6_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG['epochs']}_auc_{round(history_B6.history['auc'][CFG['epochs']-1], 2)}.h5\")\n",
    "# Serialize weights to h5:\n",
    "model_B6.save_weights(f\"./EfficientNetB6_{tfrec_shape}x{tfrec_shape}_{comp_data}_epoch{CFG['epochs']}_auc_{round(history_B6.history['auc'][CFG['epochs']-1], 2)}_weights.h5\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.034456,
     "end_time": "2020-09-21T19:18:22.711990",
     "exception": false,
     "start_time": "2020-09-21T19:18:17.677534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Deployable Format\n",
    "\n",
    "* To implement The model we need to export the model into .tflite format using Tensorflow's   python TFLite API. We can convert Saved Models as well as Keras Models From TFlite, if compatible. TFLite can then be integrated in Android Apps using MLKit and Flutter.\n",
    "* If there maybe a problem in deployment a viable course of action is to train the TFLite compatible Effnet Models in TFHub, and fine tune and then save them.\n",
    "* The TFlite models is big so we will implement Pruning and Quantisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.12553,
     "end_time": "2020-09-21T19:18:32.943967",
     "exception": false,
     "start_time": "2020-09-21T19:18:27.818437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EFFNET B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:18:43.166396Z",
     "iopub.status.busy": "2020-09-21T19:18:43.165672Z",
     "iopub.status.idle": "2020-09-21T19:20:08.841317Z",
     "shell.execute_reply": "2020-09-21T19:20:08.840644Z"
    },
    "papermill": {
     "duration": 90.816001,
     "end_time": "2020-09-21T19:20:08.841477",
     "exception": false,
     "start_time": "2020-09-21T19:18:38.025476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_B5)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model_B5 = converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_pruned_model_B5 = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:20:19.040285Z",
     "iopub.status.busy": "2020-09-21T19:20:19.039399Z",
     "iopub.status.idle": "2020-09-21T19:20:19.132375Z",
     "shell.execute_reply": "2020-09-21T19:20:19.131709Z"
    },
    "papermill": {
     "duration": 5.171587,
     "end_time": "2020-09-21T19:20:19.132506",
     "exception": false,
     "start_time": "2020-09-21T19:20:13.960919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113249064"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"pruned_model_B5.tflite\", \"wb\").write(pruned_tflite_model_B5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:20:29.448317Z",
     "iopub.status.busy": "2020-09-21T19:20:29.447559Z",
     "iopub.status.idle": "2020-09-21T19:20:29.473763Z",
     "shell.execute_reply": "2020-09-21T19:20:29.473044Z"
    },
    "papermill": {
     "duration": 5.24977,
     "end_time": "2020-09-21T19:20:29.473887",
     "exception": false,
     "start_time": "2020-09-21T19:20:24.224117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28902672"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"quantized_pruned_model_B5.tflite\", \"wb\").write(quantized_pruned_model_B5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.058024,
     "end_time": "2020-09-21T19:20:39.629587",
     "exception": false,
     "start_time": "2020-09-21T19:20:34.571563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EFFNET B6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:20:49.992749Z",
     "iopub.status.busy": "2020-09-21T19:20:49.991544Z",
     "iopub.status.idle": "2020-09-21T19:20:49.996762Z",
     "shell.execute_reply": "2020-09-21T19:20:49.996188Z"
    },
    "papermill": {
     "duration": 5.175302,
     "end_time": "2020-09-21T19:20:49.996895",
     "exception": false,
     "start_time": "2020-09-21T19:20:44.821593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_B6)\\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\\npruned_tflite_model_B6 = converter.convert()\\n\\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\nquantized_pruned_model_B6 = converter.convert()\\nopen(\"pruned_model_B6.tflite\", \"wb\").write(pruned_tflite_model_B6)\\nopen(\"quantized_pruned_model_B6.tflite\", \"wb\").write(quantized_pruned_model_B6)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_B6)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model_B6 = converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_pruned_model_B6 = converter.convert()\n",
    "open(\"pruned_model_B6.tflite\", \"wb\").write(pruned_tflite_model_B6)\n",
    "open(\"quantized_pruned_model_B6.tflite\", \"wb\").write(quantized_pruned_model_B6)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.220586,
     "end_time": "2020-09-21T19:21:00.226799",
     "exception": false,
     "start_time": "2020-09-21T19:20:55.006213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluating pruned and quantised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:21:10.789968Z",
     "iopub.status.busy": "2020-09-21T19:21:10.788973Z",
     "iopub.status.idle": "2020-09-21T19:21:28.644464Z",
     "shell.execute_reply": "2020-09-21T19:21:28.643760Z"
    },
    "papermill": {
     "duration": 23.171581,
     "end_time": "2020-09-21T19:21:28.644607",
     "exception": false,
     "start_time": "2020-09-21T19:21:05.473026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for i, j in enumerate(test_images):\n",
    "    test_labels.extend(j[1][0].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:21:38.942836Z",
     "iopub.status.busy": "2020-09-21T19:21:38.942006Z",
     "iopub.status.idle": "2020-09-21T19:21:38.947737Z",
     "shell.execute_reply": "2020-09-21T19:21:38.946968Z"
    },
    "papermill": {
     "duration": 5.15531,
     "end_time": "2020-09-21T19:21:38.947919",
     "exception": false,
     "start_time": "2020-09-21T19:21:33.792609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter, test_images):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on ever y image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "            #Pre-processing: add batch dimension and convert to float32 to match with\n",
    "            #the model's input data format.\n",
    "            #test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "            #interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "            # Run inference.\n",
    "            interpreter.invoke()\n",
    "\n",
    "            # Post-processing: remove batch dimension and find the digit with highest\n",
    "            # probability.\n",
    "            output = interpreter.tensor(output_index)\n",
    "            digit = np.argmax(output()[0])\n",
    "            prediction_digits.append(digit)\n",
    "            print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy, prediction_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:21:49.234452Z",
     "iopub.status.busy": "2020-09-21T19:21:49.233696Z",
     "iopub.status.idle": "2020-09-21T19:22:07.308799Z",
     "shell.execute_reply": "2020-09-21T19:22:07.307881Z"
    },
    "papermill": {
     "duration": 23.221397,
     "end_time": "2020-09-21T19:22:07.308984",
     "exception": false,
     "start_time": "2020-09-21T19:21:44.087587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EFFNET B5 - With Augmentation Accuracy\n",
      "Evaluated on 0 results so far.\n",
      "\n",
      "\n",
      "Pruned and quantized TFLite test_accuracy: 0.9270833333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"EFFNET B5 - With Augmentation Accuracy\")\n",
    "interpreter = tf.lite.Interpreter(model_content = quantized_pruned_model_B5)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy, preds = evaluate_model(interpreter, test_images)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-21T19:22:17.496896Z",
     "iopub.status.busy": "2020-09-21T19:22:17.495769Z",
     "iopub.status.idle": "2020-09-21T19:22:17.501891Z",
     "shell.execute_reply": "2020-09-21T19:22:17.501168Z"
    },
    "papermill": {
     "duration": 5.150131,
     "end_time": "2020-09-21T19:22:17.502030",
     "exception": false,
     "start_time": "2020-09-21T19:22:12.351899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"EFFNET B6 - With Augmentation Accuracy\")\\ninterpreter = tf.lite.Interpreter(model_content = quantized_pruned_model_B6)\\ninterpreter.allocate_tensors()\\n\\ntest_accuracy,preds = evaluate_model(interpreter, test_images)\\n\\nprint(\\'Pruned and quantized TFLite test_accuracy:\\', test_accuracy)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(\"EFFNET B6 - With Augmentation Accuracy\")\n",
    "interpreter = tf.lite.Interpreter(model_content = quantized_pruned_model_B6)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy,preds = evaluate_model(interpreter, test_images)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5.091316,
     "end_time": "2020-09-21T19:22:27.709856",
     "exception": false,
     "start_time": "2020-09-21T19:22:22.618540",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prediction image shape: (1,256,256,3)\n",
    "* 1 is the batch dimension and (256,256,3) is the image shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4490.283518,
   "end_time": "2020-09-21T19:22:33.246876",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-21T18:07:42.963358",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
